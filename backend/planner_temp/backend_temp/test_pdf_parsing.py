#!/usr/bin/env python3
"""
æµ‹è¯•PDFè§£æåŠŸèƒ½
"""

import asyncio
import json
import os
from services.planner_analysis import PlannerAnalysisService
from config import config

async def test_pdf_parsing():
    """æµ‹è¯•PDFè§£æåŠŸèƒ½"""
    print("ğŸš€ æµ‹è¯•PDFè§£æåŠŸèƒ½...")
    
    # åˆ›å»ºæµ‹è¯•ç®€å†å†…å®¹
    test_resume_content = """Name: XXX
Position: Software Engineer, Machine Learning â€“ Off-Search Sourcing and Relevance
Professional Summary
- 4+ years of full-lifecycle software development experience, from requirements gathering through deployment
- Expert in system design and architecture patterns for high-concurrency, low-latency distributed systems
- Proficient in machine learning model development: data preprocessing, feature engineering, model training
- Skilled at rapid iteration and Agile delivery, delivering incremental value through continuous integration & deployment
- Deep experience with data processing pipelines using Spark, Kafka and Hadoop ecosystems
- Strong command of Python, Java, Scala and SQL for both back-end services and ETL jobs

Core Skills
- Software Development & Architecture: SDLC, design patterns, reliability, scaling, microservices
- High-Concurrency Systems: gRPC, REST, asynchronous processing, load balancing, back-pressure handling
- Machine Learning & Data Science: TensorFlow, PyTorch, scikit-learn, MLflow, online/offline serving
- Data Processing: Apache Spark, Kafka, Hadoop, real-time streaming, batch ETL
- Programming Languages: Python, Java, Scala, SQL
- Cloud & DevOps: AWS (EC2, S3, EMR), Docker, Kubernetes, Terraform, CI/CD (Jenkins/GitHub Actions)
- Agile & Rapid Iteration: Scrum/Kanban, TDD, code review, CI/CD pipelines

Professional Experience
Senior Software Engineer | ABC Tech Co., Ltd.
July 2022 â€“ Present, Beijing
- Architected and designed a high-concurrency, low-latency online ad serving platform handling 100M+ QPS
- Led full software development lifecycle: requirement analysis, API design, coding standards, code reviews
- Built real-time data processing pipelines (Kafka â†’ Spark Streaming â†’ HBase) to compute and serve features
- Developed and deployed TensorFlow-based CTR prediction models; improved prediction accuracy by 7%
- Drove rapid iteration in collaboration with product and data science teams, executing A/B tests

Software Engineer | DEF Information Technology Ltd.
June 2019 â€“ June 2022, Shanghai
- Designed and implemented an offline feature computation platform processing hundreds of billions of records
- Delivered scalable ETL jobs in Python/Java with 200+ parallel tasks, cutting job runtime by 60%
- Contributed to system design for ad ranking and filtering services, improving relevance through iterative algorithms
- Maintained microservices (Spring Boot, gRPC) with CI/CD pipelines; ensured 99.9% service uptime

Selected Projects
Off-Search Ad Relevance Engine
- Role: Lead Architect & Developer
- Scope: Serve relevant ads on non-search pages (Product Detail, Home Page) at Amazon scale
- Tech: TensorFlow, Spark, Kafka, AWS Lambda, Docker, Kubernetes
- Impact: Achieved 6% uplift in off-search CTR and 8% incremental ad revenue

Real-Time CTR Prediction Pipeline
- Role: Full-Stack Engineer
- Scope: End-to-end streaming pipeline for feature computation and model inference
- Tech: Kafka, Spark Streaming, gRPC, Kubernetes, Helm
- Impact: Sustained <50 ms p99 end-to-end latency with 24/7 stability

Education
Bachelor of Science in Computer Science | Peking University
September 2015 â€“ June 2019
- Relevant Coursework: Algorithms & Data Structures, Distributed Systems, Machine Learning, Database Systems

Certifications & Honors
- AWS Certified Solutions Architect â€“ Associate
- TensorFlow Developer Certificate
- "Engineer of the Year" â€“ ABC Tech Co., Ltd. (2023)

Open Source & Community
- GitHub: https://github.com/your-username
- Contributed Spark plugins and a reference Feature Store implementation
"""
    
    # ä¿å­˜æµ‹è¯•ç®€å†æ–‡ä»¶ï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰
    test_resume_path = "test_resume.txt"
    with open(test_resume_path, 'w', encoding='utf-8') as f:
        f.write(test_resume_content)
    
    try:
        # åˆå§‹åŒ–æœåŠ¡
        planner_service = PlannerAnalysisService(config.OPENAI_API_KEY)
        
        # æµ‹è¯•æ–‡æœ¬ç®€å†è§£æ
        print("\nğŸ“„ æµ‹è¯•æ–‡æœ¬ç®€å†è§£æ...")
        resume_data = await planner_service.parse_resume(test_resume_path)
        
        print(f"\nâœ… æ–‡æœ¬ç®€å†è§£æç»“æœ:")
        print(f"  - æŠ€èƒ½: {resume_data['skills']}")
        print(f"  - å·¥ä½œç»éªŒ: {resume_data['experience_years']} å¹´")
        print(f"  - å­¦å†: {resume_data['education']}")
        print(f"  - é¡¹ç›®: {resume_data['projects']}")
        print(f"  - è¯­è¨€: {resume_data['languages']}")
        print(f"  - è®¤è¯: {resume_data['certifications']}")
        
        # æµ‹è¯•æŠ€èƒ½åŒ¹é…åˆ†æ
        print("\nğŸ¯ æµ‹è¯•æŠ€èƒ½åŒ¹é…åˆ†æ...")
        jd_description = """
        æˆ‘ä»¬æ­£åœ¨å¯»æ‰¾ä¸€ä½ç»éªŒä¸°å¯Œçš„æ•°æ®ç§‘å­¦å®¶ï¼Œè´Ÿè´£å¼€å‘å’Œç»´æŠ¤æˆ‘ä»¬çš„æœºå™¨å­¦ä¹ ç³»ç»Ÿã€‚
        
        èŒä½è¦æ±‚ï¼š
        - 3å¹´ä»¥ä¸ŠPythonå¼€å‘ç»éªŒ
        - ç†Ÿç»ƒæŒæ¡æœºå™¨å­¦ä¹ ç®—æ³•å’Œç»Ÿè®¡æ–¹æ³•
        - æœ‰åˆ†å¸ƒå¼ç³»ç»Ÿå’Œå¤§æ•°æ®å¤„ç†ç»éªŒ
        - ç†Ÿæ‚‰A/Bæµ‹è¯•å’Œå®éªŒè®¾è®¡
        - å…·å¤‡è‰¯å¥½çš„ç¼–ç¨‹è§„èŒƒå’Œè®¾è®¡æ¨¡å¼
        - æœ‰è‡ªç„¶è¯­è¨€å¤„ç†ç»éªŒä¼˜å…ˆ
        
        èŒè´£ï¼š
        - è®¾è®¡å’Œå¼€å‘æœºå™¨å­¦ä¹ æ¨¡å‹
        - è¿›è¡Œæ•°æ®åˆ†æå’Œç»Ÿè®¡å»ºæ¨¡
        - å‚ä¸ä»£ç å®¡æŸ¥å’ŒæŠ€æœ¯è®¨è®º
        - ä¸äº§å“å›¢é˜Ÿåä½œï¼Œç†è§£éœ€æ±‚å¹¶å®ç°
        """
        
        analysis_result = await planner_service.analyze_job_match(
            jd_description,
            resume_data['skills'],
            resume_data['experience_years']
        )
        
        print(f"\nğŸ“Š åŒ¹é…åº¦åˆ†æç»“æœ:")
        print(f"  - æŠ€èƒ½åŒ¹é…åº¦: {analysis_result['skill_match']}%")
        print(f"  - ç»éªŒåŒ¹é…åº¦: {analysis_result['experience_match']}%")
        print(f"  - æ•´ä½“åŒ¹é…åº¦: {analysis_result['overall_match']}%")
        print(f"  - å·®è·æ•°é‡: {len(analysis_result['gaps'])}")
        print(f"  - ä¼˜åŠ¿æ•°é‡: {len(analysis_result['strengths'])}")
        
        # æ˜¾ç¤ºå·®è·è¯¦æƒ…
        if analysis_result['gaps']:
            print("\nğŸ“‹ æŠ€èƒ½å·®è·è¯¦æƒ…:")
            for gap in analysis_result['gaps']:
                status_icon = "âŒ" if gap['status'] == 'missing' else "âš ï¸"
                print(f"  {status_icon} {gap['skill']} ({gap['priority']} priority)")
        
        # æ˜¾ç¤ºä¼˜åŠ¿è¯¦æƒ…
        if analysis_result['strengths']:
            print("\nâœ… æŠ€èƒ½ä¼˜åŠ¿è¯¦æƒ…:")
            for strength in analysis_result['strengths']:
                print(f"  âœ… {strength['skill']} (é‡è¦æ€§: {strength['importance']})")
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        test_result = {
            "resume_content": test_resume_content,
            "parsed_resume": resume_data,
            "analysis_result": analysis_result
        }
        
        with open('pdf_parsing_test_result.json', 'w', encoding='utf-8') as f:
            json.dump(test_result, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ° pdf_parsing_test_result.json")
        print("\nğŸ‰ PDFè§£ææµ‹è¯•æˆåŠŸï¼")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        if os.path.exists(test_resume_path):
            os.remove(test_resume_path)

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ§ª PDFè§£æåŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    asyncio.run(test_pdf_parsing())
    
    print("\n" + "=" * 60)
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print("=" * 60) 