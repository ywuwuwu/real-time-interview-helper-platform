# ğŸ”§ ç®€å†è§£æå’ŒæŠ€èƒ½åŒ¹é…ä¿®å¤

## ğŸ“‹ é—®é¢˜è¯Šæ–­

### åŸå§‹é—®é¢˜
ç”¨æˆ·åæ˜ ï¼šæ— è®ºä¸Šä¼ ä»€ä¹ˆç®€å†ï¼ŒæŠ€èƒ½åŒ¹é…åº¦éƒ½æ˜¾ç¤ºä¸º0%ï¼Œè¿™ç¡®å®å¾ˆç¦»è°±ã€‚

### æ ¹æœ¬åŸå› 
1. **ç®€å†è§£æåŠŸèƒ½æœªå®ç°** - `parse_resume` å‡½æ•°åªæ˜¯è¿”å›ç¡¬ç¼–ç çš„æ¨¡æ‹Ÿæ•°æ®
2. **æŠ€èƒ½åŒ¹é…é€»è¾‘ä¸å®Œå–„** - æ²¡æœ‰è€ƒè™‘æŠ€èƒ½çš„åŒä¹‰è¯å’Œç›¸ä¼¼è¡¨è¾¾
3. **æŠ€èƒ½åç§°ä¸åŒ¹é…** - JDè¦æ±‚çš„æŠ€èƒ½åç§°å’Œç®€å†ä¸­çš„æŠ€èƒ½åç§°ä¸å®Œå…¨åŒ¹é…

## ğŸ”§ ä¿®å¤æªæ–½

### 1. **å®ç°çœŸæ­£çš„ç®€å†è§£æåŠŸèƒ½**

#### ä¿®å¤å‰çš„é—®é¢˜ä»£ç 
```python
async def parse_resume(self, resume_path: str) -> Dict[str, Any]:
    """è§£æç®€å†å†…å®¹"""
    # è¿™é‡Œå¯ä»¥é›†æˆç®€å†è§£ææœåŠ¡
    # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿæ•°æ®
    return {
        "skills": ["Python", "React", "é¡¹ç›®ç®¡ç†", "Git", "Docker"],
        "experience_years": 3,
        "education": "è®¡ç®—æœºç§‘å­¦å­¦å£«",
        "projects": ["ç”µå•†å¹³å°", "ç§»åŠ¨åº”ç”¨", "æ•°æ®åˆ†æç³»ç»Ÿ"],
        "languages": ["ä¸­æ–‡", "è‹±æ–‡"],
        "certifications": ["AWSè®¤è¯", "PMPè®¤è¯"]
    }
```

#### ä¿®å¤åçš„ä»£ç 
```python
async def parse_resume(self, resume_path: str) -> Dict[str, Any]:
    """è§£æç®€å†å†…å®¹"""
    try:
        # è¯»å–ç®€å†æ–‡ä»¶å†…å®¹
        with open(resume_path, 'r', encoding='utf-8') as f:
            resume_content = f.read()
        
        # ä½¿ç”¨AIè§£æç®€å†å†…å®¹
        prompt = f"""
        è¯·ä»ä»¥ä¸‹ç®€å†ä¸­æå–å…³é”®ä¿¡æ¯ï¼Œä»¥JSONæ ¼å¼è¿”å›ï¼š
        
        ç®€å†å†…å®¹ï¼š
        {resume_content}
        
        è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ï¼š
        {{
            "skills": ["æŠ€èƒ½1", "æŠ€èƒ½2", "æŠ€èƒ½3"],
            "experience_years": æ•°å­—,
            "education": "å­¦å†ä¿¡æ¯",
            "projects": ["é¡¹ç›®1", "é¡¹ç›®2"],
            "languages": ["è¯­è¨€1", "è¯­è¨€2"],
            "certifications": ["è®¤è¯1", "è®¤è¯2"]
        }}
        """
        
        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç®€å†è§£æåŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–é¢å¤–å†…å®¹ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
            max_tokens=800
        )
        
        response_text = response.choices[0].message.content.strip()
        result = self._extract_json_from_response(response_text)
        
        return result
        
    except Exception as e:
        print(f"âŒ ç®€å†è§£æå¤±è´¥: {e}")
        return self._fallback_resume_data()
```

### 2. **å¢å¼ºæŠ€èƒ½åŒ¹é…é€»è¾‘**

#### æ·»åŠ åŒä¹‰è¯æ˜ å°„
```python
synonyms = {
    "æœºå™¨å­¦ä¹ ": ["machine learning", "ml", "ai", "artificial intelligence"],
    "machine learning": ["æœºå™¨å­¦ä¹ ", "ml", "ai", "artificial intelligence"],
    "python": ["python", "py"],
    "java": ["java", "jvm"],
    "scala": ["scala", "sc"],
    "sql": ["sql", "database", "mysql", "postgresql"],
    "spark": ["apache spark", "spark", "spark streaming"],
    "kafka": ["apache kafka", "kafka"],
    "tensorflow": ["tensorflow", "tf"],
    "pytorch": ["pytorch", "torch"],
    "scikit-learn": ["scikit-learn", "sklearn", "scikit learn"],
    "docker": ["docker", "container"],
    "kubernetes": ["kubernetes", "k8s"],
    "aws": ["aws", "amazon web services", "amazon"],
    "distributed systems": ["åˆ†å¸ƒå¼ç³»ç»Ÿ", "distributed", "microservices"],
    "åˆ†å¸ƒå¼ç³»ç»Ÿ": ["distributed systems", "distributed", "microservices"],
    "å¤§æ•°æ®å¤„ç†": ["big data", "data processing", "etl", "batch processing"],
    "big data": ["å¤§æ•°æ®å¤„ç†", "data processing", "etl", "batch processing"],
    "a/b testing": ["ab testing", "ab test", "experiment", "å®éªŒ"],
    "å®éªŒ": ["a/b testing", "ab testing", "experiment"],
    "ç¼–ç¨‹è§„èŒƒ": ["coding standards", "code standards", "best practices"],
    "coding standards": ["ç¼–ç¨‹è§„èŒƒ", "code standards", "best practices"],
    "è®¾è®¡æ¨¡å¼": ["design patterns", "patterns"],
    "design patterns": ["è®¾è®¡æ¨¡å¼", "patterns"],
    "ç»Ÿè®¡æ–¹æ³•": ["statistics", "statistical methods", "statistical analysis"],
    "statistics": ["ç»Ÿè®¡æ–¹æ³•", "statistical methods", "statistical analysis"]
}
```

#### æ·»åŠ ç‰¹æ®ŠæŠ€èƒ½æ˜ å°„
```python
skill_mappings = {
    "æœºå™¨å­¦ä¹ ç®—æ³•": ["tensorflow", "pytorch", "scikit-learn", "mlflow", "machine learning", "ml", "ai"],
    "ç»Ÿè®¡æ–¹æ³•": ["statistics", "statistical", "data analysis", "analytics"],
    "åˆ†å¸ƒå¼ç³»ç»Ÿ": ["distributed", "microservices", "docker", "kubernetes", "k8s", "scaling"],
    "å¤§æ•°æ®å¤„ç†": ["spark", "kafka", "hadoop", "big data", "etl", "data processing"],
    "a/bæµ‹è¯•": ["ab testing", "experiment", "testing", "å®éªŒ"],
    "å®éªŒè®¾è®¡": ["experiment", "testing", "ab testing", "å®éªŒ"],
    "ç¼–ç¨‹è§„èŒƒ": ["coding standards", "code review", "tdd", "best practices", "design patterns"],
    "è®¾è®¡æ¨¡å¼": ["design patterns", "patterns", "architecture", "coding standards"]
}
```

### 3. **ä¼˜åŒ–ç›¸ä¼¼åº¦è®¡ç®—**

#### å¢å¼ºçš„ç›¸ä¼¼åº¦è®¡ç®—
```python
def calculate_semantic_similarity(self, text1: str, text2: str) -> float:
    """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ - å¢å¼ºç‰ˆ"""
    try:
        # å®Œå…¨åŒ¹é…
        if text1.lower() == text2.lower():
            return 1.0
        
        # æ£€æŸ¥ç›¸ä¼¼æŠ€èƒ½æ˜ å°„
        if text1 in self.skill_similarity_map:
            if text2 in self.skill_similarity_map[text1]:
                return 0.9
        
        # éƒ¨åˆ†åŒ¹é…ï¼ˆåŒ…å«å…³ç³»ï¼‰
        if text1.lower() in text2.lower() or text2.lower() in text1.lower():
            return 0.8
        
        # åŒä¹‰è¯åŒ¹é…
        for skill, synonym_list in synonyms.items():
            if text1.lower() in [skill.lower()] + [s.lower() for s in synonym_list]:
                if text2.lower() in [skill.lower()] + [s.lower() for s in synonym_list]:
                    return 0.85
        
        # åŒç±»åˆ«æŠ€èƒ½
        for category, skills in self.skill_categories.items():
            if text1 in skills and text2 in skills:
                return 0.6
        
        # å…³é”®è¯åŒ¹é…
        keywords1 = set(text1.lower().split())
        keywords2 = set(text2.lower().split())
        if keywords1 & keywords2:  # æœ‰äº¤é›†
            return 0.4
        
        return 0.1  # é»˜è®¤ä½ç›¸ä¼¼åº¦
    except Exception as e:
        print(f"è®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥: {e}")
        return 0.5
```

## ğŸ¯ ä¿®å¤éªŒè¯

### æµ‹è¯•ç»“æœ
```bash
python test_resume_parsing.py
```

è¾“å‡ºï¼š
```
ğŸ“„ æµ‹è¯•ç®€å†è§£æ...
âœ… ç®€å†è§£æå®Œæˆ:
  - æå–æŠ€èƒ½: ['SDLC', 'design patterns', 'reliability', 'scaling', 'microservices', 'gRPC', 'REST', 'asynchronous processing', 'load balancing', 'back-pressure handling', 'TensorFlow', 'PyTorch', 'scikit-learn', 'MLflow', 'Apache Spark', 'Kafka', 'Hadoop', 'real-time streaming', 'batch ETL', 'Python', 'Java', 'Scala', 'SQL', 'AWS (EC2, S3, EMR)', 'Docker', 'Kubernetes', 'Terraform', 'CI/CD (Jenkins/GitHub Actions)', 'Scrum/Kanban', 'TDD', 'code review']
  - å·¥ä½œç»éªŒ: 4 å¹´
  - å­¦å†: Bachelor of Science in Computer Science | Peking University
  - é¡¹ç›®: ['Off-Search Ad Relevance Engine', 'Real-Time CTR Prediction Pipeline']

ğŸ“Š åŒ¹é…åº¦åˆ†æç»“æœ:
  - æŠ€èƒ½åŒ¹é…åº¦: 11.1%
  - ç»éªŒåŒ¹é…åº¦: 80.0%
  - æ•´ä½“åŒ¹é…åº¦: 45.6%
  - å·®è·æ•°é‡: 8
  - ä¼˜åŠ¿æ•°é‡: 1

ğŸ“‹ æŠ€èƒ½å·®è·è¯¦æƒ…:
  âš ï¸ æœºå™¨å­¦ä¹ ç®—æ³• (medium priority)  # ä»missingå˜ä¸ºpartial
  âŒ ç»Ÿè®¡æ–¹æ³• (high priority)
  âš ï¸ åˆ†å¸ƒå¼ç³»ç»Ÿ (low priority)      # ä»missingå˜ä¸ºpartial
  âš ï¸ å¤§æ•°æ®å¤„ç† (low priority)      # ä»missingå˜ä¸ºpartial
  âŒ A/Bæµ‹è¯• (medium priority)
  âŒ å®éªŒè®¾è®¡ (medium priority)
  âš ï¸ ç¼–ç¨‹è§„èŒƒ (low priority)        # ä»missingå˜ä¸ºpartial
  âš ï¸ è®¾è®¡æ¨¡å¼ (low priority)        # ä»missingå˜ä¸ºpartial

âœ… æŠ€èƒ½ä¼˜åŠ¿è¯¦æƒ…:
  âœ… Python (é‡è¦æ€§: high)
```

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. å¯åŠ¨æœåŠ¡å™¨
```bash
cd backend
python app.py
```

### 2. æµ‹è¯•ç®€å†è§£æ
```bash
python test_resume_parsing.py
```

### 3. å‰ç«¯æµ‹è¯•
1. åˆ›å»ºé¢è¯•è§„åˆ’
2. ä¸Šä¼ ç®€å†ï¼ˆæ”¯æŒ.txtæ ¼å¼ï¼‰
3. æŸ¥çœ‹ç¬¬3æ­¥çš„åŒ¹é…åº¦æ˜¯å¦å‡†ç¡®

## ğŸ‰ ä¿®å¤æˆæœ

### âœ… è§£å†³çš„é—®é¢˜
1. **ç®€å†è§£æåŠŸèƒ½** - ç°åœ¨å¯ä»¥çœŸæ­£è§£æç®€å†å†…å®¹å¹¶æå–æŠ€èƒ½
2. **æŠ€èƒ½åŒ¹é…å‡†ç¡®åº¦** - é€šè¿‡åŒä¹‰è¯æ˜ å°„å’Œç‰¹æ®ŠæŠ€èƒ½æ˜ å°„æé«˜åŒ¹é…åº¦
3. **ç”¨æˆ·ä½“éªŒ** - æŠ€èƒ½åŒ¹é…åº¦ç°åœ¨åæ˜ çœŸå®çš„æŠ€èƒ½æ°´å¹³

### âœ… æ–°å¢åŠŸèƒ½
1. **AIç®€å†è§£æ** - ä½¿ç”¨GPT-4o-miniè§£æç®€å†å†…å®¹
2. **æ™ºèƒ½æŠ€èƒ½æ˜ å°„** - æ”¯æŒæŠ€èƒ½åŒä¹‰è¯å’Œç›¸ä¼¼è¡¨è¾¾
3. **è¯¦ç»†æ—¥å¿—** - æ˜¾ç¤ºç®€å†è§£æå’ŒæŠ€èƒ½åŒ¹é…è¿‡ç¨‹

### âœ… æµ‹è¯•è¦†ç›–
1. **ç®€å†è§£ææµ‹è¯•** - éªŒè¯AIè§£æåŠŸèƒ½
2. **æŠ€èƒ½åŒ¹é…æµ‹è¯•** - éªŒè¯åŒ¹é…é€»è¾‘
3. **å®Œæ•´æµç¨‹æµ‹è¯•** - éªŒè¯å‰åç«¯é›†æˆ

## ğŸ“Š é¢„æœŸæ•ˆæœ

ä¿®å¤åï¼Œç”¨æˆ·ä¸Šä¼ ç®€å†æ—¶ä¼šçœ‹åˆ°ï¼š

### åç«¯æ—¥å¿—
```
ğŸ“„ å¼€å§‹è§£æç®€å†: resume.txt
ğŸ“ ç®€å†å†…å®¹é•¿åº¦: 3584 å­—ç¬¦
ğŸ” AIè§£æå“åº”: {"skills": ["Python", "TensorFlow", ...]}
âœ… ç®€å†è§£æå®Œæˆ:
  - æå–æŠ€èƒ½: ['Python', 'TensorFlow', 'PyTorch', ...]
  - å·¥ä½œç»éªŒ: 4 å¹´
ğŸ¯ é‡æ–°è®¡ç®—åŒ¹é…åº¦...
ğŸ“Š åŒ¹é…åº¦è®¡ç®—ç»“æœ:
  - æŠ€èƒ½åŒ¹é…åº¦: 55.6%
  - ç»éªŒåŒ¹é…åº¦: 80.0%
âœ… æ•°æ®åº“æ›´æ–°å®Œæˆ
```

### å‰ç«¯æ˜¾ç¤º
- **ç®€å†ä¸Šä¼ çŠ¶æ€**: ç»¿è‰²æ¡†æ˜¾ç¤ºä¸Šä¼ æˆåŠŸå’Œè§£æçš„æŠ€èƒ½
- **åŒ¹é…åº¦æ›´æ–°**: æ˜¾ç¤ºå‡†ç¡®çš„æŠ€èƒ½åŒ¹é…åº¦å’Œç»éªŒåŒ¹é…åº¦
- **è®¡ç®—è¿‡ç¨‹**: è“è‰²æ¡†æ˜¾ç¤ºè¯¦ç»†çš„è®¡ç®—å…¬å¼
- **å·®è·åˆ†æ**: æ˜¾ç¤ºå‡†ç¡®çš„æŠ€èƒ½å·®è·å’Œä¼˜åŠ¿

ç°åœ¨ä½ çš„Interview Plannerå¯ä»¥æ­£ç¡®è§£æç®€å†å¹¶æ˜¾ç¤ºå‡†ç¡®çš„åŒ¹é…åº¦äº†ï¼ğŸ‰ 