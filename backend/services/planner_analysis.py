import openai
from typing import Dict, List, Any, Optional
import json
import os
import re
from datetime import datetime

class PlannerAnalysisService:
    def __init__(self, openai_api_key: str):
        self.client = openai.OpenAI(api_key=openai_api_key)
        
        # é¢„å®šä¹‰çš„æŠ€èƒ½åˆ†ç±»
        self.skill_categories = {
            "programming": ["Python", "Java", "JavaScript", "C++", "Go", "Rust", "TypeScript"],
            "frontend": ["React", "Vue", "Angular", "HTML", "CSS", "JavaScript", "TypeScript"],
            "backend": ["Node.js", "Django", "Flask", "Spring", "Express", "FastAPI"],
            "database": ["MySQL", "PostgreSQL", "MongoDB", "Redis", "Elasticsearch"],
            "cloud": ["AWS", "Azure", "GCP", "Docker", "Kubernetes", "Terraform"],
            "mobile": ["React Native", "Flutter", "iOS", "Android", "Swift", "Kotlin"],
            "ai_ml": ["TensorFlow", "PyTorch", "Scikit-learn", "Pandas", "NumPy"],
            "devops": ["Jenkins", "GitLab CI", "GitHub Actions", "Ansible", "Docker"],
            "soft_skills": ["é¡¹ç›®ç®¡ç†", "å›¢é˜Ÿåä½œ", "æ²Ÿé€šèƒ½åŠ›", "é¢†å¯¼åŠ›", "é—®é¢˜è§£å†³"]
        }
        
        # æŠ€èƒ½ç›¸ä¼¼åº¦æ˜ å°„
        self.skill_similarity_map = {
            "React": ["Vue", "Angular", "JavaScript"],
            "Vue": ["React", "Angular", "JavaScript"],
            "Angular": ["React", "Vue", "JavaScript"],
            "JavaScript": ["TypeScript", "React", "Vue"],
            "TypeScript": ["JavaScript", "React", "Vue"],
            "Python": ["Java", "C++", "Go"],
            "Java": ["Python", "C++", "Go"],
            "C++": ["Python", "Java", "Go"],
            "MySQL": ["PostgreSQL", "MongoDB", "Redis"],
            "PostgreSQL": ["MySQL", "MongoDB", "Redis"],
            "MongoDB": ["MySQL", "PostgreSQL", "Redis"],
            "AWS": ["Azure", "GCP", "Docker"],
            "Azure": ["AWS", "GCP", "Docker"],
            "GCP": ["AWS", "Azure", "Docker"],
            "Docker": ["Kubernetes", "AWS", "Azure"],
            "Kubernetes": ["Docker", "AWS", "Azure"],
            "é¡¹ç›®ç®¡ç†": ["å›¢é˜Ÿåä½œ", "æ²Ÿé€šèƒ½åŠ›", "é¢†å¯¼åŠ›"],
            "å›¢é˜Ÿåä½œ": ["é¡¹ç›®ç®¡ç†", "æ²Ÿé€šèƒ½åŠ›", "é¢†å¯¼åŠ›"],
            "æ²Ÿé€šèƒ½åŠ›": ["é¡¹ç›®ç®¡ç†", "å›¢é˜Ÿåä½œ", "é¢†å¯¼åŠ›"]
        }
    
    def _extract_json_from_response(self, response_text: str) -> Dict[str, Any]:
        """ä»APIå“åº”ä¸­æå–JSON"""
        try:
            # å°è¯•ç›´æ¥è§£æ
            return json.loads(response_text)
        except json.JSONDecodeError:
            # å°è¯•æå–JSONéƒ¨åˆ†
            try:
                # æŸ¥æ‰¾JSONå¼€å§‹å’Œç»“æŸçš„ä½ç½®
                start = response_text.find('{')
                end = response_text.rfind('}') + 1
                if start != -1 and end != 0:
                    json_str = response_text[start:end]
                    return json.loads(json_str)
            except:
                pass
            
            # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œè¿”å›é»˜è®¤ç»“æ„
            return {
                "required_skills": [],
                "preferred_skills": [],
                "experience_requirements": []
            }
    
    def extract_skills_from_jd(self, job_description: str) -> List[Dict[str, Any]]:
        """ä»JDä¸­æå–æŠ€èƒ½è¦æ±‚"""
        prompt = f"""
        è¯·ä»ä»¥ä¸‹èŒä½æè¿°ä¸­æå–æ‰€æœ‰æŠ€èƒ½è¦æ±‚ï¼Œå¹¶æŒ‰é‡è¦æ€§åˆ†ç±»ã€‚
        
        èŒä½æè¿°ï¼š
        {job_description}
        
        è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ï¼š
        {{
            "required_skills": [
                {{"skill": "Python", "importance": "high", "category": "programming"}},
                {{"skill": "React", "importance": "medium", "category": "frontend"}},
                {{"skill": "é¡¹ç›®ç®¡ç†", "importance": "low", "category": "soft_skills"}}
            ],
            "preferred_skills": [
                {{"skill": "Docker", "importance": "medium", "category": "devops"}}
            ],
            "experience_requirements": [
                {{"type": "æŠ€æœ¯ç»éªŒ", "years": 3, "description": "åç«¯å¼€å‘ç»éªŒ"}},
                {{"type": "ç®¡ç†ç»éªŒ", "years": 1, "description": "å›¢é˜Ÿç®¡ç†ç»éªŒ"}}
            ]
        }}
        """
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€èƒ½åˆ†æåŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–é¢å¤–å†…å®¹ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=1000
            )
            
            response_text = response.choices[0].message.content.strip()
            print(f"APIå“åº”: {response_text[:200]}...")  # è°ƒè¯•ç”¨
            
            result = self._extract_json_from_response(response_text)
            return result
        except Exception as e:
            print(f"æå–æŠ€èƒ½å¤±è´¥: {e}")
            return self._fallback_skill_extraction(job_description)
    
    def _fallback_skill_extraction(self, job_description: str) -> Dict[str, Any]:
        """å¤‡ç”¨æŠ€èƒ½æå–æ–¹æ³•"""
        skills = []
        for category, skill_list in self.skill_categories.items():
            for skill in skill_list:
                if skill.lower() in job_description.lower():
                    skills.append({
                        "skill": skill,
                        "importance": "medium",
                        "category": category
                    })
        
        return {
            "required_skills": skills[:5],
            "preferred_skills": skills[5:8] if len(skills) > 5 else [],
            "experience_requirements": [
                {"type": "æŠ€æœ¯ç»éªŒ", "years": 3, "description": "ç›¸å…³æŠ€æœ¯ç»éªŒ"}
            ]
        }
    
    def calculate_semantic_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ - å¢å¼ºç‰ˆ"""
        try:
            # å®Œå…¨åŒ¹é…
            if text1.lower() == text2.lower():
                return 1.0
            
            # æ£€æŸ¥ç›¸ä¼¼æŠ€èƒ½æ˜ å°„
            if text1 in self.skill_similarity_map:
                if text2 in self.skill_similarity_map[text1]:
                    return 0.9
            
            if text2 in self.skill_similarity_map:
                if text1 in self.skill_similarity_map[text2]:
                    return 0.9
            
            # éƒ¨åˆ†åŒ¹é…ï¼ˆåŒ…å«å…³ç³»ï¼‰
            if text1.lower() in text2.lower() or text2.lower() in text1.lower():
                return 0.8
            
            # åŒä¹‰è¯åŒ¹é…
            synonyms = {
                "æœºå™¨å­¦ä¹ ": ["machine learning", "ml", "ai", "artificial intelligence"],
                "machine learning": ["æœºå™¨å­¦ä¹ ", "ml", "ai", "artificial intelligence"],
                "python": ["python", "py"],
                "java": ["java", "jvm"],
                "scala": ["scala", "sc"],
                "sql": ["sql", "database", "mysql", "postgresql"],
                "spark": ["apache spark", "spark", "spark streaming"],
                "kafka": ["apache kafka", "kafka"],
                "tensorflow": ["tensorflow", "tf"],
                "pytorch": ["pytorch", "torch"],
                "scikit-learn": ["scikit-learn", "sklearn", "scikit learn"],
                "docker": ["docker", "container"],
                "kubernetes": ["kubernetes", "k8s"],
                "aws": ["aws", "amazon web services", "amazon"],
                "distributed systems": ["åˆ†å¸ƒå¼ç³»ç»Ÿ", "distributed", "microservices"],
                "åˆ†å¸ƒå¼ç³»ç»Ÿ": ["distributed systems", "distributed", "microservices"],
                "å¤§æ•°æ®å¤„ç†": ["big data", "data processing", "etl", "batch processing"],
                "big data": ["å¤§æ•°æ®å¤„ç†", "data processing", "etl", "batch processing"],
                "a/b testing": ["ab testing", "ab test", "experiment", "å®éªŒ"],
                "å®éªŒ": ["a/b testing", "ab testing", "experiment"],
                "ç¼–ç¨‹è§„èŒƒ": ["coding standards", "code standards", "best practices"],
                "coding standards": ["ç¼–ç¨‹è§„èŒƒ", "code standards", "best practices"],
                "è®¾è®¡æ¨¡å¼": ["design patterns", "patterns"],
                "design patterns": ["è®¾è®¡æ¨¡å¼", "patterns"],
                "ç»Ÿè®¡æ–¹æ³•": ["statistics", "statistical methods", "statistical analysis"],
                "statistics": ["ç»Ÿè®¡æ–¹æ³•", "statistical methods", "statistical analysis"]
            }
            
            # æ£€æŸ¥åŒä¹‰è¯
            for skill, synonym_list in synonyms.items():
                if text1.lower() in [skill.lower()] + [s.lower() for s in synonym_list]:
                    if text2.lower() in [skill.lower()] + [s.lower() for s in synonym_list]:
                        return 0.85
            
            # åŒç±»åˆ«æŠ€èƒ½
            for category, skills in self.skill_categories.items():
                if text1 in skills and text2 in skills:
                    return 0.6
            
            # å…³é”®è¯åŒ¹é…
            keywords1 = set(text1.lower().split())
            keywords2 = set(text2.lower().split())
            if keywords1 & keywords2:  # æœ‰äº¤é›†
                return 0.4
            
            return 0.1  # é»˜è®¤ä½ç›¸ä¼¼åº¦
        except Exception as e:
            print(f"è®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return 0.5
    
    def analyze_skill_gaps(
        self, 
        jd_skills: List[Dict], 
        user_skills: List[str]
    ) -> Dict[str, Any]:
        """åˆ†ææŠ€èƒ½å·®è· - å¢å¼ºç‰ˆ"""
        gaps = []
        strengths = []
        missing_skills = []
        
        # åˆ›å»ºç”¨æˆ·æŠ€èƒ½çš„æ ‡å‡†åŒ–ç‰ˆæœ¬
        normalized_user_skills = []
        for skill in user_skills:
            normalized_user_skills.append(skill.lower())
            # æ·»åŠ æŠ€èƒ½çš„åŒä¹‰è¯
            if skill.lower() in ["tensorflow", "tf"]:
                normalized_user_skills.extend(["machine learning", "ml", "ai"])
            elif skill.lower() in ["pytorch", "torch"]:
                normalized_user_skills.extend(["machine learning", "ml", "ai"])
            elif skill.lower() in ["scikit-learn", "sklearn"]:
                normalized_user_skills.extend(["machine learning", "ml", "ai"])
            elif skill.lower() in ["apache spark", "spark"]:
                normalized_user_skills.extend(["big data", "data processing", "etl"])
            elif skill.lower() in ["kafka"]:
                normalized_user_skills.extend(["big data", "data processing", "streaming"])
            elif skill.lower() in ["hadoop"]:
                normalized_user_skills.extend(["big data", "data processing", "distributed"])
            elif skill.lower() in ["docker", "kubernetes", "k8s"]:
                normalized_user_skills.extend(["distributed systems", "microservices"])
            elif skill.lower() in ["aws", "amazon web services"]:
                normalized_user_skills.extend(["cloud", "distributed systems"])
            elif skill.lower() in ["design patterns", "patterns"]:
                normalized_user_skills.extend(["coding standards", "best practices"])
            elif skill.lower() in ["code review", "tdd"]:
                normalized_user_skills.extend(["coding standards", "best practices"])
        
        # åˆ†ææ¯ä¸ªJDæŠ€èƒ½
        for jd_skill in jd_skills:
            skill_name = jd_skill["skill"]
            importance = jd_skill["importance"]
            normalized_jd_skill = skill_name.lower()
            
            # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æœ‰æ­¤æŠ€èƒ½
            if skill_name in user_skills:
                strengths.append({
                    "skill": skill_name,
                    "importance": importance,
                    "category": jd_skill.get("category", "unknown"),
                    "status": "strong"
                })
            else:
                # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œçœ‹æ˜¯å¦æœ‰ç›¸ä¼¼æŠ€èƒ½
                max_similarity = 0
                similar_skill = None
                
                # æ£€æŸ¥æ ‡å‡†åŒ–æŠ€èƒ½
                for user_skill in normalized_user_skills:
                    similarity = self.calculate_semantic_similarity(normalized_jd_skill, user_skill)
                    if similarity > max_similarity:
                        max_similarity = similarity
                        similar_skill = user_skill
                
                # æ£€æŸ¥åŸå§‹æŠ€èƒ½
                for user_skill in user_skills:
                    similarity = self.calculate_semantic_similarity(skill_name, user_skill)
                    if similarity > max_similarity:
                        max_similarity = similarity
                        similar_skill = user_skill
                
                # ç‰¹æ®ŠæŠ€èƒ½æ˜ å°„æ£€æŸ¥
                skill_mappings = {
                    "æœºå™¨å­¦ä¹ ç®—æ³•": ["tensorflow", "pytorch", "scikit-learn", "mlflow", "machine learning", "ml", "ai", "æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ", "machine learning"],
                    "ç»Ÿè®¡æ–¹æ³•": ["statistics", "statistical", "data analysis", "analytics", "ç»Ÿè®¡", "æ•°æ®åˆ†æ", "statistics"],
                    "åˆ†å¸ƒå¼ç³»ç»Ÿ": ["distributed", "microservices", "docker", "kubernetes", "k8s", "scaling", "åˆ†å¸ƒå¼", "å¾®æœåŠ¡", "distributed systems"],
                    "å¤§æ•°æ®å¤„ç†": ["spark", "kafka", "hadoop", "big data", "etl", "data processing", "å¤§æ•°æ®", "æ•°æ®å¤„ç†", "data pipelines"],
                    "a/bæµ‹è¯•": ["ab testing", "experiment", "testing", "å®éªŒ", "a/b test", "a/b testing"],
                    "å®éªŒè®¾è®¡": ["experiment", "testing", "ab testing", "å®éªŒ", "å®éªŒè®¾è®¡"],
                    "ç¼–ç¨‹è§„èŒƒ": ["coding standards", "code review", "tdd", "best practices", "design patterns", "ç¼–ç¨‹è§„èŒƒ", "ä»£ç è§„èŒƒ"],
                    "è®¾è®¡æ¨¡å¼": ["design patterns", "patterns", "architecture", "coding standards", "è®¾è®¡æ¨¡å¼", "æ¶æ„æ¨¡å¼"],
                    "software development": ["python", "java", "scala", "programming", "coding", "è½¯ä»¶å¼€å‘", "ç¼–ç¨‹", "software development"],
                    "machine learning": ["tensorflow", "pytorch", "scikit-learn", "ml", "ai", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "machine learning"],
                    "data pipelines": ["spark", "kafka", "hadoop", "etl", "data processing", "æ•°æ®ç®¡é“", "æ•°æ®å¤„ç†", "data pipelines"],
                    "distributed systems": ["docker", "kubernetes", "microservices", "åˆ†å¸ƒå¼", "å¾®æœåŠ¡", "distributed systems"],
                    "coding standards": ["code review", "tdd", "best practices", "ç¼–ç¨‹è§„èŒƒ", "ä»£ç è§„èŒƒ"],
                    "design patterns": ["patterns", "architecture", "è®¾è®¡æ¨¡å¼", "æ¶æ„æ¨¡å¼"],
                    "a/b testing": ["experiment", "testing", "å®éªŒ", "ab test", "a/b testing"],
                    "statistics": ["statistical", "data analysis", "analytics", "ç»Ÿè®¡", "æ•°æ®åˆ†æ", "statistics"],
                    "information retrieval": ["search", "elasticsearch", "solr", "ä¿¡æ¯æ£€ç´¢", "æœç´¢", "information retrieval"],
                    "natural language processing": ["nlp", "text processing", "language model", "è‡ªç„¶è¯­è¨€å¤„ç†", "æ–‡æœ¬å¤„ç†"],
                    "system design": ["architecture", "design", "ç³»ç»Ÿè®¾è®¡", "æ¶æ„è®¾è®¡", "system design"],
                    "programming languages": ["python", "java", "scala", "programming", "ç¼–ç¨‹è¯­è¨€", "programming languages"]
                }
                
                # æ£€æŸ¥ç‰¹æ®Šæ˜ å°„
                if skill_name in skill_mappings:
                    for mapped_skill in skill_mappings[skill_name]:
                        if mapped_skill in normalized_user_skills:
                            max_similarity = 0.8
                            similar_skill = mapped_skill
                            break
                
                if max_similarity > 0.6:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                    gaps.append({
                        "skill": skill_name,
                        "similar_skill": similar_skill,
                        "similarity": max_similarity,
                        "importance": importance,
                        "category": jd_skill.get("category", "unknown"),
                        "status": "partial",
                        "priority": "medium" if importance == "high" else "low"
                    })
                else:
                    gaps.append({
                        "skill": skill_name,
                        "importance": importance,
                        "category": jd_skill.get("category", "unknown"),
                        "status": "missing",
                        "priority": "high" if importance == "high" else "medium"
                    })
                    missing_skills.append(skill_name)
        
        return {
            "gaps": gaps,
            "strengths": strengths,
            "missing_skills": missing_skills,
            "gap_count": len(gaps),
            "strength_count": len(strengths)
        }
    
    async def analyze_job_match(
        self, 
        job_description: str, 
        user_skills: List[str], 
        experience_years: int
    ) -> Dict[str, Any]:
        """åˆ†æJDä¸ç”¨æˆ·èƒ½åŠ›çš„åŒ¹é…åº¦ - å¢å¼ºç‰ˆ"""
        
        # 1. æå–JDæŠ€èƒ½è¦æ±‚
        jd_analysis = self.extract_skills_from_jd(job_description)
        
        # 2. åˆ†ææŠ€èƒ½å·®è·
        skill_analysis = self.analyze_skill_gaps(
            jd_analysis["required_skills"], 
            user_skills
        )
        
        # 3. è®¡ç®—åŒ¹é…åº¦
        total_skills = len(jd_analysis["required_skills"])
        matched_skills = len(skill_analysis["strengths"])
        skill_match_percentage = (matched_skills / total_skills * 100) if total_skills > 0 else 0
        
        # 4. ç»éªŒåŒ¹é…åˆ†æ
        experience_requirements = jd_analysis.get("experience_requirements", [])
        experience_match_percentage = self._calculate_experience_match(
            experience_requirements, experience_years
        )
        
        # 5. ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š
        detailed_analysis = await self._generate_detailed_analysis(
            job_description, user_skills, experience_years, skill_analysis, jd_analysis
        )
        
        return {
            "skill_match": round(skill_match_percentage, 1),
            "experience_match": round(experience_match_percentage, 1),
            "overall_match": round((skill_match_percentage + experience_match_percentage) / 2, 1),
            "gaps": skill_analysis["gaps"],
            "strengths": skill_analysis["strengths"],
            "missing_skills": skill_analysis["missing_skills"],
            "jd_requirements": jd_analysis,
            "detailed_analysis": detailed_analysis,
            "improvement_priorities": self._generate_improvement_priorities(skill_analysis),
            "timeline_estimate": self._estimate_improvement_timeline(skill_analysis),
            "confidence_score": self._calculate_confidence_score(skill_analysis, experience_years)
        }
    
    def _calculate_experience_match(
        self, 
        requirements: List[Dict], 
        user_years: int
    ) -> float:
        """è®¡ç®—ç»éªŒåŒ¹é…åº¦"""
        if not requirements:
            return 70.0  # é»˜è®¤åŒ¹é…åº¦
        
        total_required_years = sum(req.get("years", 0) for req in requirements)
        avg_required_years = total_required_years / len(requirements)
        
        if user_years >= avg_required_years:
            return min(100.0, 70 + (user_years - avg_required_years) * 10)
        else:
            return max(0.0, 70 - (avg_required_years - user_years) * 15)
    
    async def _generate_detailed_analysis(
        self, 
        job_description: str, 
        user_skills: List[str], 
        experience_years: int,
        skill_analysis: Dict,
        jd_analysis: Dict
    ) -> Dict[str, Any]:
        """ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š"""
        
        prompt = f"""
        åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼Œç”Ÿæˆè¯¦ç»†çš„èŒä½åŒ¹é…åˆ†ææŠ¥å‘Šã€‚
        
        èŒä½æè¿°ï¼š{job_description}
        ç”¨æˆ·æŠ€èƒ½ï¼š{', '.join(user_skills)}
        å·¥ä½œç»éªŒï¼š{experience_years}å¹´
        
        æŠ€èƒ½å·®è·åˆ†æï¼š{json.dumps(skill_analysis, ensure_ascii=False)}
        JDè¦æ±‚åˆ†æï¼š{json.dumps(jd_analysis, ensure_ascii=False)}
        
        è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ï¼š
        {{
            "core_competencies": ["æŠ€èƒ½1", "æŠ€èƒ½2"],
            "main_gaps": ["å·®è·1", "å·®è·2"],
            "short_term_goals": ["ç›®æ ‡1", "ç›®æ ‡2"],
            "medium_term_goals": ["ç›®æ ‡1", "ç›®æ ‡2"],
            "long_term_goals": ["ç›®æ ‡1", "ç›®æ ‡2"],
            "risk_assessment": "é£é™©è¯„ä¼°æè¿°",
            "market_positioning": "å¸‚åœºå®šä½å»ºè®®"
        }}
        """
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èŒä¸šåˆ†æåŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–é¢å¤–å†…å®¹ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=800
            )
            
            response_text = response.choices[0].message.content.strip()
            print(f"è¯¦ç»†åˆ†æAPIå“åº”: {response_text[:200]}...")  # è°ƒè¯•ç”¨
            
            result = self._extract_json_from_response(response_text)
            return result
        except Exception as e:
            print(f"ç”Ÿæˆè¯¦ç»†åˆ†æå¤±è´¥: {e}")
            return self._fallback_detailed_analysis(skill_analysis)
    
    def _fallback_detailed_analysis(self, skill_analysis: Dict) -> Dict[str, Any]:
        """å¤‡ç”¨è¯¦ç»†åˆ†æ"""
        return {
            "core_competencies": ["æŠ€æœ¯èƒ½åŠ›", "é—®é¢˜è§£å†³"],
            "main_gaps": ["æŠ€èƒ½å·®è·", "ç»éªŒä¸è¶³"],
            "short_term_goals": ["å­¦ä¹ æ–°æŠ€èƒ½", "é¡¹ç›®å®è·µ"],
            "medium_term_goals": ["æŠ€èƒ½æå‡", "ç»éªŒç§¯ç´¯"],
            "long_term_goals": ["èŒä¸šå‘å±•", "æŠ€èƒ½æ·±åŒ–"],
            "risk_assessment": "éœ€è¦æŒç»­å­¦ä¹ å’Œå®è·µ",
            "market_positioning": "æŠ€æœ¯ä¸“å®¶æ–¹å‘"
        }
    
    def _generate_improvement_priorities(self, skill_analysis: Dict) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ”¹è¿›ä¼˜å…ˆçº§"""
        priorities = []
        
        for gap in skill_analysis["gaps"]:
            priority_score = 0
            if gap["importance"] == "high":
                priority_score += 3
            elif gap["importance"] == "medium":
                priority_score += 2
            else:
                priority_score += 1
            
            if gap["status"] == "missing":
                priority_score += 2
            elif gap["status"] == "partial":
                priority_score += 1
            
            priorities.append({
                "skill": gap["skill"],
                "priority_score": priority_score,
                "estimated_time": "2-4å‘¨" if gap["status"] == "partial" else "1-3ä¸ªæœˆ",
                "learning_path": self._generate_learning_path(gap["skill"], gap["category"])
            })
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        priorities.sort(key=lambda x: x["priority_score"], reverse=True)
        return priorities[:5]  # è¿”å›å‰5ä¸ªä¼˜å…ˆçº§
    
    def _generate_learning_path(self, skill: str, category: str) -> List[str]:
        """ç”Ÿæˆå­¦ä¹ è·¯å¾„"""
        learning_paths = {
            "programming": [
                f"å­¦ä¹ {skill}åŸºç¡€è¯­æ³•",
                f"å®Œæˆ{skill}å°é¡¹ç›®",
                f"å‚ä¸{skill}å¼€æºé¡¹ç›®"
            ],
            "frontend": [
                f"å­¦ä¹ {skill}æ¡†æ¶åŸºç¡€",
                f"æ„å»º{skill}é¡¹ç›®",
                f"ä¼˜åŒ–{skill}æ€§èƒ½"
            ],
            "backend": [
                f"å­¦ä¹ {skill}æ¡†æ¶",
                f"æ„å»ºAPIæœåŠ¡",
                f"éƒ¨ç½²{skill}åº”ç”¨"
            ],
            "database": [
                f"å­¦ä¹ {skill}åŸºç¡€",
                f"è®¾è®¡æ•°æ®åº“æ¶æ„",
                f"ä¼˜åŒ–{skill}æ€§èƒ½"
            ],
            "cloud": [
                f"å­¦ä¹ {skill}åŸºç¡€æ¦‚å¿µ",
                f"å®Œæˆ{skill}è®¤è¯",
                f"å®è·µ{skill}é¡¹ç›®"
            ]
        }
        
        return learning_paths.get(category, [
            f"å­¦ä¹ {skill}åŸºç¡€",
            f"å®è·µ{skill}é¡¹ç›®",
            f"æ·±å…¥{skill}é«˜çº§ç‰¹æ€§"
        ])
    
    def _estimate_improvement_timeline(self, skill_analysis: Dict) -> Dict[str, Any]:
        """ä¼°ç®—æ”¹è¿›æ—¶é—´çº¿"""
        high_priority_gaps = [gap for gap in skill_analysis["gaps"] if gap["priority"] == "high"]
        medium_priority_gaps = [gap for gap in skill_analysis["gaps"] if gap["priority"] == "medium"]
        
        total_weeks = len(high_priority_gaps) * 8 + len(medium_priority_gaps) * 4
        
        return {
            "total_weeks": total_weeks,
            "high_priority_weeks": len(high_priority_gaps) * 8,
            "medium_priority_weeks": len(medium_priority_gaps) * 4,
            "estimated_completion_date": self._calculate_completion_date(total_weeks),
            "milestones": self._generate_milestones(skill_analysis["gaps"])
        }
    
    def _calculate_completion_date(self, weeks: int) -> str:
        """è®¡ç®—é¢„è®¡å®Œæˆæ—¥æœŸ"""
        from datetime import datetime, timedelta
        completion_date = datetime.now() + timedelta(weeks=weeks)
        return completion_date.strftime("%Y-%m-%d")
    
    def _generate_milestones(self, gaps: List[Dict]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆé‡Œç¨‹ç¢‘"""
        milestones = []
        current_week = 0
        
        for gap in gaps:
            weeks_needed = 8 if gap["priority"] == "high" else 4
            current_week += weeks_needed
            
            milestones.append({
                "skill": gap["skill"],
                "week": current_week,
                "description": f"æŒæ¡{gap['skill']}æŠ€èƒ½",
                "priority": gap["priority"]
            })
        
        return milestones
    
    def _calculate_confidence_score(self, skill_analysis: Dict, experience_years: int) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°"""
        # åŸºäºæŠ€èƒ½å·®è·æ•°é‡å’Œç»éªŒå¹´æ•°è®¡ç®—ç½®ä¿¡åº¦
        gap_count = len(skill_analysis["gaps"])
        strength_count = len(skill_analysis["strengths"])
        
        # åŸºç¡€åˆ†æ•°
        base_score = 70.0
        
        # æŠ€èƒ½åŒ¹é…åº¦å½±å“
        if strength_count > gap_count:
            base_score += 15
        elif gap_count > strength_count * 2:
            base_score -= 20
        
        # ç»éªŒå½±å“
        if experience_years >= 5:
            base_score += 10
        elif experience_years < 2:
            base_score -= 10
        
        return max(0.0, min(100.0, base_score))

    async def generate_recommendations(
        self, 
        analysis_result: Dict[str, Any]
    ) -> Dict[str, List[Dict[str, Any]]]:
        """ç”Ÿæˆä¸ªæ€§åŒ–æ¨è - åŸºäºçœŸå®æŠ€èƒ½å·®è·åˆ†æ"""
        
        # æå–å…³é”®ä¿¡æ¯
        gaps = analysis_result.get('gaps', [])
        strengths = analysis_result.get('strengths', [])
        skill_match = analysis_result.get('skill_match', 0)
        experience_match = analysis_result.get('experience_match', 0)
        
        # æŒ‰ä¼˜å…ˆçº§åˆ†ç±»æŠ€èƒ½å·®è·
        high_priority_gaps = [gap for gap in gaps if gap.get('priority') == 'high']
        medium_priority_gaps = [gap for gap in gaps if gap.get('priority') == 'medium']
        low_priority_gaps = [gap for gap in gaps if gap.get('priority') == 'low']
        
        prompt = f"""
        åŸºäºä»¥ä¸‹è¯¦ç»†çš„æŠ€èƒ½åˆ†æç»“æœï¼Œç”Ÿæˆä¸ªæ€§åŒ–çš„å­¦ä¹ å’Œå‘å±•å»ºè®®ã€‚
        
        åˆ†æç»“æœï¼š
        - æŠ€èƒ½åŒ¹é…åº¦: {skill_match}%
        - ç»éªŒåŒ¹é…åº¦: {experience_match}%
        - æ•´ä½“åŒ¹é…åº¦: {analysis_result.get('overall_match', 0)}%
        
        æŠ€èƒ½å·®è·åˆ†æï¼š
        - é«˜ä¼˜å…ˆçº§å·®è· ({len(high_priority_gaps)}é¡¹): {[gap['skill'] for gap in high_priority_gaps]}
        - ä¸­ä¼˜å…ˆçº§å·®è· ({len(medium_priority_gaps)}é¡¹): {[gap['skill'] for gap in medium_priority_gaps]}
        - ä½ä¼˜å…ˆçº§å·®è· ({len(low_priority_gaps)}é¡¹): {[gap['skill'] for gap in low_priority_gaps]}
        
        æŠ€èƒ½ä¼˜åŠ¿ï¼š
        - åŒ¹é…æŠ€èƒ½ ({len(strengths)}é¡¹): {[strength['skill'] for strength in strengths]}
        
        è¯·åŸºäºä»¥ä¸Šåˆ†æï¼Œç”Ÿæˆé’ˆå¯¹æ€§çš„æ¨èï¼Œé‡ç‚¹å…³æ³¨ï¼š
        1. ä¼˜å…ˆæ¨èé«˜ä¼˜å…ˆçº§æŠ€èƒ½å·®è·çš„è¯¾ç¨‹
        2. é¡¹ç›®ç»ƒä¹ åº”è¯¥ç»“åˆç”¨æˆ·ç°æœ‰æŠ€èƒ½å’Œéœ€è¦æå‡çš„æŠ€èƒ½
        3. æ¨¡æ‹Ÿé¢è¯•åº”è¯¥é’ˆå¯¹ç›®æ ‡å²—ä½çš„å…·ä½“è¦æ±‚
        4. è€ƒè™‘ç”¨æˆ·çš„ç»éªŒæ°´å¹³å’Œå­¦ä¹ æ—¶é—´
        
        è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ï¼š
        {{
            "courses": [
                {{
                    "id": "course_1",
                    "name": "è¯¾ç¨‹åç§°",
                    "platform": "å¹³å°åç§°",
                    "difficulty": "åˆçº§/ä¸­çº§/é«˜çº§",
                    "duration": "å­¦ä¹ æ—¶é•¿",
                    "url": "è¯¾ç¨‹é“¾æ¥",
                    "description": "è¯¾ç¨‹æè¿°",
                    "target_skill": "ç›®æ ‡æŠ€èƒ½",
                    "priority": "high/medium/low"
                }}
            ],
            "projects": [
                {{
                    "id": "project_1",
                    "name": "é¡¹ç›®åç§°",
                    "tech_stack": ["æŠ€æœ¯æ ˆ"],
                    "difficulty": "åˆçº§/ä¸­çº§/é«˜çº§",
                    "duration": "é¡¹ç›®æ—¶é•¿",
                    "description": "é¡¹ç›®æè¿°",
                    "learning_objectives": ["å­¦ä¹ ç›®æ ‡"],
                    "target_skills": ["ç›®æ ‡æŠ€èƒ½"]
                }}
            ],
            "practice": [
                {{
                    "id": "practice_1",
                    "type": "ç»ƒä¹ ç±»å‹",
                    "frequency": "ç»ƒä¹ é¢‘ç‡",
                    "focus": "é‡ç‚¹å†…å®¹",
                    "description": "ç»ƒä¹ æè¿°",
                    "target_skills": ["ç›®æ ‡æŠ€èƒ½"]
                }}
            ],
            "learning_path": {{
                "short_term": ["çŸ­æœŸç›®æ ‡"],
                "medium_term": ["ä¸­æœŸç›®æ ‡"],
                "long_term": ["é•¿æœŸç›®æ ‡"]
            }},
            "timeline": {{
                "estimated_weeks": æ•°å­—,
                "milestones": ["é‡Œç¨‹ç¢‘"]
            }}
        }}
        
        æ³¨æ„ï¼š
        1. è¯¾ç¨‹åº”è¯¥é’ˆå¯¹å…·ä½“çš„æŠ€èƒ½å·®è·
        2. é¡¹ç›®åº”è¯¥ç»“åˆç”¨æˆ·ç°æœ‰æŠ€èƒ½å’Œæ–°æŠ€èƒ½
        3. ç»ƒä¹ åº”è¯¥é’ˆå¯¹é¢è¯•è¦æ±‚
        4. å­¦ä¹ è·¯å¾„åº”è¯¥æœ‰æ˜ç¡®çš„é˜¶æ®µæ€§ç›®æ ‡
        5. æ—¶é—´çº¿è¦ç°å®å¯è¡Œ
        """
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èŒä¸šå‘å±•é¡¾é—®å’ŒæŠ€èƒ½æå‡ä¸“å®¶ã€‚è¯·åŸºäºç”¨æˆ·çš„æŠ€èƒ½å·®è·åˆ†æï¼Œç”Ÿæˆå…·ä½“ã€å¯æ‰§è¡Œçš„ä¸ªæ€§åŒ–æ¨èã€‚ç¡®ä¿æ¨èå†…å®¹é’ˆå¯¹æ€§å¼ºã€å®ç”¨æ€§å¼ºã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                max_tokens=1500
            )
            
            response_text = response.choices[0].message.content.strip()
            print(f"ğŸ” AIæ¨èç”Ÿæˆå“åº”: {response_text[:300]}...")
            
            result = self._extract_json_from_response(response_text)
            
            # ç¡®ä¿è¿”å›çš„æ•°æ®ç»“æ„æ­£ç¡®
            if not result.get("courses"):
                result["courses"] = []
            if not result.get("projects"):
                result["projects"] = []
            if not result.get("practice"):
                result["practice"] = []
            if not result.get("learning_path"):
                result["learning_path"] = {"short_term": [], "medium_term": [], "long_term": []}
            if not result.get("timeline"):
                result["timeline"] = {"estimated_weeks": 12, "milestones": []}
            
            print(f"âœ… ä¸ªæ€§åŒ–æ¨èç”Ÿæˆå®Œæˆ:")
            print(f"  - æ¨èè¯¾ç¨‹: {len(result['courses'])} é¡¹")
            print(f"  - æ¨èé¡¹ç›®: {len(result['projects'])} é¡¹")
            print(f"  - æ¨èç»ƒä¹ : {len(result['practice'])} é¡¹")
            print(f"  - å­¦ä¹ è·¯å¾„: {len(result['learning_path']['short_term'])} çŸ­æœŸç›®æ ‡")
            print(f"  - é¢„è®¡æ—¶é—´: {result['timeline']['estimated_weeks']} å‘¨")
            
            return result
            
        except Exception as e:
            print(f"âŒ AIæ¨èç”Ÿæˆå¤±è´¥: {e}")
            # è¿”å›åŸºäºæŠ€èƒ½å·®è·çš„æ™ºèƒ½é»˜è®¤æ¨è
            return self._generate_smart_fallback_recommendations(analysis_result)
    
    def _generate_smart_fallback_recommendations(self, analysis_result: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ™ºèƒ½çš„å¤‡ç”¨æ¨è"""
        gaps = analysis_result.get('gaps', [])
        skill_match = analysis_result.get('skill_match', 0)
        
        # åŸºäºæŠ€èƒ½å·®è·ç”Ÿæˆæ¨è
        courses = []
        projects = []
        practice = []
        
        # è¯¾ç¨‹æ¨è
        if any('machine learning' in gap['skill'].lower() for gap in gaps):
            courses.append({
                "id": "course_ml",
                "name": "æœºå™¨å­¦ä¹ åŸºç¡€ - Coursera",
                "platform": "Coursera",
                "difficulty": "ä¸­çº§",
                "duration": "8å‘¨",
                "url": "https://www.coursera.org/learn/machine-learning",
                "description": "å´æ©è¾¾æ•™æˆçš„ç»å…¸æœºå™¨å­¦ä¹ è¯¾ç¨‹",
                "target_skill": "Machine Learning",
                "priority": "high"
            })
        
        if any('python' in gap['skill'].lower() for gap in gaps):
            courses.append({
                "id": "course_python",
                "name": "Pythonç¼–ç¨‹åŸºç¡€ - Codecademy",
                "platform": "Codecademy",
                "difficulty": "åˆçº§",
                "duration": "3å‘¨",
                "url": "https://www.codecademy.com/learn/learn-python-3",
                "description": "ä»é›¶å¼€å§‹å­¦ä¹ Pythonç¼–ç¨‹",
                "target_skill": "Python",
                "priority": "high"
            })
        
        if any('system design' in gap['skill'].lower() for gap in gaps):
            courses.append({
                "id": "course_system_design",
                "name": "ç³»ç»Ÿè®¾è®¡é¢è¯•å‡†å¤‡ - Educative",
                "platform": "Educative",
                "difficulty": "é«˜çº§",
                "duration": "6å‘¨",
                "url": "https://www.educative.io/courses/grokking-the-system-design-interview",
                "description": "ä¸“é—¨é’ˆå¯¹ç³»ç»Ÿè®¾è®¡é¢è¯•çš„è¯¾ç¨‹",
                "target_skill": "System Design",
                "priority": "high"
            })
        
        # é¡¹ç›®æ¨è
        if skill_match < 50:
            projects.append({
                "id": "project_basic",
                "name": "å…¨æ ˆWebåº”ç”¨å¼€å‘",
                "tech_stack": ["React", "Node.js", "MongoDB"],
                "difficulty": "ä¸­çº§",
                "duration": "4-6å‘¨",
                "description": "å¼€å‘ä¸€ä¸ªå®Œæ•´çš„Webåº”ç”¨ï¼Œæ¶µç›–å‰åç«¯å¼€å‘",
                "learning_objectives": ["æŒæ¡å…¨æ ˆå¼€å‘", "å­¦ä¹ æ•°æ®åº“è®¾è®¡", "ç†è§£APIå¼€å‘"],
                "target_skills": ["React", "Node.js", "MongoDB"]
            })
        
        # ç»ƒä¹ æ¨è
        practice.append({
            "id": "practice_coding",
            "type": "ç¼–ç¨‹ç»ƒä¹ ",
            "frequency": "æ¯å‘¨3æ¬¡",
            "focus": "ç®—æ³•å’Œæ•°æ®ç»“æ„",
            "description": "åœ¨LeetCodeä¸Šç»ƒä¹ ç¼–ç¨‹é¢˜ï¼Œé‡ç‚¹ç»ƒä¹ ç›®æ ‡å²—ä½ç›¸å…³çš„ç®—æ³•",
            "target_skills": ["ç®—æ³•", "æ•°æ®ç»“æ„", "ç¼–ç¨‹"]
        })
        
        practice.append({
            "id": "practice_interview",
            "type": "æ¨¡æ‹Ÿé¢è¯•",
            "frequency": "æ¯å‘¨1æ¬¡",
            "focus": "æŠ€æœ¯é¢è¯•å’Œç³»ç»Ÿè®¾è®¡",
            "description": "æ¨¡æ‹ŸçœŸå®é¢è¯•ç¯å¢ƒï¼Œç»ƒä¹ æŠ€æœ¯é—®é¢˜å›ç­”",
            "target_skills": ["é¢è¯•æŠ€å·§", "æŠ€æœ¯è¡¨è¾¾", "ç³»ç»Ÿè®¾è®¡"]
        })
        
        return {
            "courses": courses,
            "projects": projects,
            "practice": practice,
            "learning_path": {
                "short_term": ["æŒæ¡åŸºç¡€ç¼–ç¨‹æŠ€èƒ½", "å­¦ä¹ æ ¸å¿ƒç®—æ³•"],
                "medium_term": ["å®Œæˆå®æˆ˜é¡¹ç›®", "æå‡ç³»ç»Ÿè®¾è®¡èƒ½åŠ›"],
                "long_term": ["è¾¾åˆ°ç›®æ ‡å²—ä½è¦æ±‚", "å‡†å¤‡é¢è¯•"]
            },
            "timeline": {
                "estimated_weeks": 12,
                "milestones": ["ç¬¬4å‘¨å®ŒæˆåŸºç¡€è¯¾ç¨‹", "ç¬¬8å‘¨å®Œæˆé¡¹ç›®", "ç¬¬12å‘¨å‡†å¤‡é¢è¯•"]
            }
        }
    
    async def parse_resume(self, resume_path: str) -> Dict[str, Any]:
        """è§£æç®€å†å†…å®¹ - ç®€åŒ–ç‰ˆï¼Œç›´æ¥è®©GPTåˆ†æ"""
        try:
            print(f"ğŸ“„ å¼€å§‹è§£æç®€å†: {resume_path}")
            
            # 1. æå–ç®€å†æ–‡æœ¬å†…å®¹
            resume_content = await self._extract_resume_text(resume_path)
            if not resume_content:
                error_msg = f"âŒ ç®€å†æ–‡æœ¬æå–å¤±è´¥: {resume_path}"
                print(error_msg)
                print("ğŸ’¡ å»ºè®®:")
                print("  1. ç¡®ä¿PDFæ–‡ä»¶æ²¡æœ‰æŸå")
                print("  2. å°è¯•å°†PDFè½¬æ¢ä¸ºæ–‡æœ¬æ–‡ä»¶")
                print("  3. æ£€æŸ¥æ–‡ä»¶ç¼–ç æ ¼å¼")
                print("  4. ç¡®ä¿PyPDF2åº“å·²æ­£ç¡®å®‰è£…: pip install PyPDF2==3.0.1")
                raise ValueError(error_msg)
            
            print(f"ğŸ“ ç®€å†å†…å®¹é•¿åº¦: {len(resume_content)} å­—ç¬¦")
            
            # æ£€æŸ¥å†…å®¹æ˜¯å¦å¤ªçŸ­
            if len(resume_content.strip()) < 50:
                error_msg = f"âŒ ç®€å†å†…å®¹è¿‡çŸ­ï¼Œå¯èƒ½æå–å¤±è´¥: {len(resume_content)} å­—ç¬¦"
                print(error_msg)
                print("ğŸ’¡ å»ºè®®:")
                print("  1. æ£€æŸ¥PDFæ–‡ä»¶æ˜¯å¦åŒ…å«æ–‡æœ¬å†…å®¹")
                print("  2. å°è¯•ä½¿ç”¨å…¶ä»–PDFé˜…è¯»å™¨æ‰“å¼€æ–‡ä»¶")
                print("  3. å°†PDFè½¬æ¢ä¸ºæ–‡æœ¬æ–‡ä»¶åé‡è¯•")
                raise ValueError(error_msg)
            
            # 2. ç›´æ¥è®©GPTåˆ†ææ•´ä¸ªç®€å†
            prompt = f"""
            è¯·åˆ†æä»¥ä¸‹ç®€å†ï¼Œæå–å…³é”®ä¿¡æ¯å¹¶è¿›è¡Œè¯¦ç»†çš„æŠ€èƒ½åˆ†æã€‚

            ç®€å†å†…å®¹ï¼š
            {resume_content}

            è¯·æä¾›ä»¥ä¸‹åˆ†æï¼š

            1. æŠ€èƒ½æ¸…å•ï¼ˆåŒ…æ‹¬æŠ€æœ¯æŠ€èƒ½ã€è½¯æŠ€èƒ½ã€å·¥å…·ç­‰ï¼‰
            2. å·¥ä½œç»éªŒå¹´æ•°
            3. æ•™è‚²èƒŒæ™¯
            4. é¡¹ç›®ç»éªŒ
            5. æŠ€èƒ½åˆ†ç±»ï¼ˆç¼–ç¨‹è¯­è¨€ã€æ¡†æ¶ã€å·¥å…·ã€è½¯æŠ€èƒ½ç­‰ï¼‰
            6. æŠ€èƒ½ç†Ÿç»ƒåº¦è¯„ä¼°ï¼ˆåˆçº§/ä¸­çº§/é«˜çº§ï¼‰

            è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
            {{
                "skills": ["æŠ€èƒ½1", "æŠ€èƒ½2", "æŠ€èƒ½3"],
                "experience_years": æ•°å­—,
                "education": "å­¦å†ä¿¡æ¯",
                "projects": ["é¡¹ç›®1", "é¡¹ç›®2"],
                "languages": ["è¯­è¨€1", "è¯­è¨€2"],
                "certifications": ["è®¤è¯1", "è®¤è¯2"],
                "skill_categories": {{
                    "programming_languages": ["Python", "Java"],
                    "frameworks": ["React", "Django"],
                    "tools": ["Git", "Docker"],
                    "soft_skills": ["é¡¹ç›®ç®¡ç†", "å›¢é˜Ÿåä½œ"]
                }},
                "skill_levels": {{
                    "Python": "é«˜çº§",
                    "React": "ä¸­çº§",
                    "é¡¹ç›®ç®¡ç†": "é«˜çº§"
                }},
                "detailed_analysis": "è¯¦ç»†çš„æŠ€èƒ½åˆ†æè¯´æ˜"
            }}

            æ³¨æ„ï¼š
            1. æå–æ‰€æœ‰ç›¸å…³çš„æŠ€æœ¯æŠ€èƒ½ï¼ŒåŒ…æ‹¬ç¼–ç¨‹è¯­è¨€ã€æ¡†æ¶ã€å·¥å…·ã€å¹³å°ç­‰
            2. å‡†ç¡®è®¡ç®—å·¥ä½œç»éªŒå¹´æ•°
            3. å¯¹æŠ€èƒ½è¿›è¡Œåˆç†åˆ†ç±»
            4. è¯„ä¼°æŠ€èƒ½ç†Ÿç»ƒåº¦
            5. æä¾›è¯¦ç»†çš„åˆ†æè¯´æ˜
            """
            
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç®€å†åˆ†æä¸“å®¶ã€‚è¯·ä»”ç»†åˆ†æç®€å†å†…å®¹ï¼Œæå–å‡†ç¡®çš„ä¿¡æ¯ï¼Œå¹¶æä¾›è¯¦ç»†çš„æŠ€èƒ½åˆ†æã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=1500
            )
            
            response_text = response.choices[0].message.content.strip()
            print(f"ğŸ” GPTåˆ†æå“åº”: {response_text[:300]}...")
            
            result = self._extract_json_from_response(response_text)
            
            # ç¡®ä¿è¿”å›çš„æ•°æ®ç»“æ„æ­£ç¡®
            if not result.get("skills"):
                result["skills"] = []
            if not result.get("experience_years"):
                result["experience_years"] = 0
            if not result.get("education"):
                result["education"] = "æœªæŒ‡å®š"
            if not result.get("projects"):
                result["projects"] = []
            if not result.get("languages"):
                result["languages"] = ["ä¸­æ–‡", "è‹±æ–‡"]
            if not result.get("certifications"):
                result["certifications"] = []
            if not result.get("skill_categories"):
                result["skill_categories"] = {}
            if not result.get("skill_levels"):
                result["skill_levels"] = {}
            if not result.get("detailed_analysis"):
                result["detailed_analysis"] = "æŠ€èƒ½åˆ†æå®Œæˆ"
            
            print(f"âœ… ç®€å†è§£æå®Œæˆ:")
            print(f"  - æå–æŠ€èƒ½: {result['skills']}")
            print(f"  - å·¥ä½œç»éªŒ: {result['experience_years']} å¹´")
            print(f"  - å­¦å†: {result['education']}")
            print(f"  - é¡¹ç›®: {result['projects']}")
            print(f"  - æŠ€èƒ½åˆ†ç±»: {result.get('skill_categories', {})}")
            print(f"  - æŠ€èƒ½ç­‰çº§: {result.get('skill_levels', {})}")
            print(f"  - è¯¦ç»†åˆ†æ: {result.get('detailed_analysis', '')[:100]}...")
            
            return result
            
        except Exception as e:
            error_msg = f"âŒ ç®€å†è§£æå¤±è´¥: {e}"
            print(error_msg)
            print("ğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
            print("  1. æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ”¯æŒ")
            print("  2. ç¡®ä¿æ–‡ä»¶æ²¡æœ‰æŸå")
            print("  3. å°è¯•ä½¿ç”¨æ–‡æœ¬æ ¼å¼çš„ç®€å†")
            print("  4. æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIå¯†é’¥")
            raise ValueError(error_msg)
    
    async def _extract_resume_text(self, resume_path: str) -> str:
        """æå–ç®€å†æ–‡æœ¬å†…å®¹"""
        try:
            file_extension = resume_path.lower().split('.')[-1]
            
            if file_extension == 'pdf':
                print("ğŸ“„ æ£€æµ‹åˆ°PDFæ–‡ä»¶ï¼Œæå–æ–‡æœ¬...")
                try:
                    import PyPDF2
                    resume_content = ""
                    with open(resume_path, 'rb') as file:
                        pdf_reader = PyPDF2.PdfReader(file)
                        print(f"âœ… PDFæ–‡ä»¶è¯»å–æˆåŠŸï¼Œé¡µæ•°: {len(pdf_reader.pages)}")
                        for i, page in enumerate(pdf_reader.pages):
                            page_text = page.extract_text()
                            resume_content += page_text + "\n"
                            print(f"âœ… ç¬¬{i+1}é¡µæå–æˆåŠŸï¼Œé•¿åº¦: {len(page_text)} å­—ç¬¦")
                    print(f"âœ… PDFæ–‡æœ¬æå–æˆåŠŸï¼Œæ€»é•¿åº¦: {len(resume_content)} å­—ç¬¦")
                    return resume_content
                except ImportError as e:
                    print(f"âš ï¸ PyPDF2å¯¼å…¥å¤±è´¥: {e}")
                    return None
                except Exception as e:
                    print(f"âŒ PDFè§£æå¤±è´¥: {e}")
                    return None
            else:
                # å¯¹äºæ–‡æœ¬æ–‡ä»¶ï¼Œå°è¯•å¤šç§ç¼–ç 
                encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']
                
                for encoding in encodings:
                    try:
                        with open(resume_path, 'r', encoding=encoding) as f:
                            resume_content = f.read()
                        print(f"âœ… ä½¿ç”¨ {encoding} ç¼–ç æˆåŠŸè¯»å–æ–‡ä»¶")
                        return resume_content
                    except UnicodeDecodeError:
                        print(f"âš ï¸ {encoding} ç¼–ç å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ª...")
                        continue
                
                print("âŒ æ‰€æœ‰ç¼–ç éƒ½å¤±è´¥")
                return None
                
        except Exception as e:
            print(f"âŒ æ–‡æœ¬æå–å¤±è´¥: {e}")
            return None
    
    def _get_default_resume_data(self) -> Dict[str, Any]:
        """è¿”å›é»˜è®¤çš„ç®€å†æ•°æ®"""
        return {
            "skills": ["Python", "React", "é¡¹ç›®ç®¡ç†", "Git", "Docker"],
            "experience_years": 3,
            "education": "è®¡ç®—æœºç§‘å­¦å­¦å£«",
            "projects": ["ç”µå•†å¹³å°", "ç§»åŠ¨åº”ç”¨", "æ•°æ®åˆ†æç³»ç»Ÿ"],
            "languages": ["ä¸­æ–‡", "è‹±æ–‡"],
            "certifications": ["AWSè®¤è¯", "PMPè®¤è¯"]
        } 